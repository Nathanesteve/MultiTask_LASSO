\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%
\section*{De l'estimateur des moindres carrés au MultiTask LASSO} \vspace{.5cm}
%%%%%%%%%%%%%%%%%%%%%

\textbf{Méthode des moindres carrés ordinaire.} \\
\\ Le problème moindres carrés consiste à minimiser la fonction suivante: \\
$$\underset{\theta\in R^p}{\mathrm{argmin}} \ f(\theta)=\frac{1}{2} {\| y - X\theta\|_2^2} \
, \ y \in R^p,\ X \in \mathcal{M}(n,p).
 $$
 
La fonction $f$ étant convexe et différentiable, nous calculons le gradient de $f$ et l'annulons pour trouver son minimum global.
Le gradient de $f$ est donné par l'expression suivante\ :

\begin{align*}
    \nabla f(x) = \begin{pmatrix}
        \frac{\partial f}{\partial \theta_1} (\theta) \\ \vdots \\ \frac{\partial f}{\partial \theta_p} (\theta)
    \end{pmatrix}= \begin{pmatrix}
        x_1^T(X \theta -y) \\ \vdots \\ x_p^T(X \theta -y)
    \end{pmatrix} =X^T(X \theta -y)
\end{align*}

Soit $j \in \{1, ... ,p \}.$
	 
\begin{align*}
    \frac{\partial f}{\partial \theta_j} (\theta) = 0     &\Longleftrightarrow x_j^T(X \theta -y)= 0\\
     &\Longleftrightarrow x_j^T(x_j\theta_j + \sum_{k \ne j} x_k\theta_k - y) = 0\\ &\Longleftrightarrow \theta_j = \frac{x_j^T(y -  \sum_{k \ne j} x_k\theta_k)}{x_j^Tx_j} \\ &\Longleftrightarrow \theta_j = \frac{x_j^T(y -  \sum_{k=1}^{p} x_k\theta_k  + x_j\theta_j)}{\| x_j\|_2^2}
\end{align*} \\

La dernière écriture de $\theta_j$ va nous permettre, par un algorithme de descente par coordonnés, d'estimer le paramètre $\theta$ solution du problème de minimisation.



\end{document}

